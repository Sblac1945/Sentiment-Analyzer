{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa7b4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be6f4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {TOKEN}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
    "    return r\n",
    "\n",
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fd5bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"AAAAAAAAAAAAAAAAAAAAAG7mpQEAAAAAZV18lqOK6%2BuKZUi%2BsSDBNNuNwK0%3D8kMF2LrDDmyX99oo7Bwv7GbxPKrmh8uwr0ZHge4ylN23xS6QX6\"\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
    "\n",
    "\n",
    "# Optional params: start_time,end_time,since_id,until_id,max_results,next_token,\n",
    "# expansions,tweet.fields,media.fields,poll.fields,place.fields,user.fields\n",
    "query_params = {'max_results' : 100, 'query': 'shooting OR Uvalde OR gun OR gunman OR shooter OR constitution OR #guncontrol OR #ndamendment OR #freedom',\n",
    "                'tweet.fields': 'text,lang,public_metrics,geo,author_id'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c50bbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "(429, '{\"account_id\":1692690256015904768,\"product_name\":\"standard-basic\",\"title\":\"UsageCapExceeded\",\"period\":\"Monthly\",\"scope\":\"Product\",\"detail\":\"Usage cap exceeded: Monthly product cap\",\"type\":\"https://api.twitter.com/2/problems/usage-capped\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m): \u001b[38;5;66;03m# change back to 100 and time back 60\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m         json_response \u001b[38;5;241m=\u001b[39m connect_to_endpoint(search_url, query_params)\n\u001b[0;32m      7\u001b[0m         data \u001b[38;5;241m=\u001b[39m json_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m         result_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[31], line 14\u001b[0m, in \u001b[0;36mconnect_to_endpoint\u001b[1;34m(url, params)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mException\u001b[0m: (429, '{\"account_id\":1692690256015904768,\"product_name\":\"standard-basic\",\"title\":\"UsageCapExceeded\",\"period\":\"Monthly\",\"scope\":\"Product\",\"detail\":\"Usage cap exceeded: Monthly product cap\",\"type\":\"https://api.twitter.com/2/problems/usage-capped\"}')"
     ]
    }
   ],
   "source": [
    "\n",
    "# before for loop make a big data frame, inside loop concatenate existing big frame with each little frame, pickle after loop\n",
    "# remember to get rid of duplicates if necessary\n",
    "big_data = pd.DataFrame(columns = ['author_id', 'lang', 'text', 'retweet_count', 'reply_count', 'like_count', 'quote_count'])\n",
    "for i in range(10):\n",
    "    for a in range(100): # change back to 100 and time back 60\n",
    "        json_response = connect_to_endpoint(search_url, query_params)\n",
    "        data = json_response[\"data\"]\n",
    "        result_dict = {}\n",
    "        b = 0\n",
    "        for x in data:\n",
    "            result_dict[i] = {'author_id':x['id'],'lang':x['lang'],'text':x['text'],\\\n",
    "                               'retweet_count':x['public_metrics']['retweet_count'],\\\n",
    "                             'reply_count':x['public_metrics']['reply_count'],\\\n",
    "                             'like_count':x['public_metrics']['like_count'],\\\n",
    "                             'quote_count':x['public_metrics']['quote_count'],}\n",
    "            b+=1\n",
    "        #     how to concatenate it in this case and make it work with the function from_dict?\n",
    "\n",
    "        result_df = pd.DataFrame.from_dict(result_dict, orient=\"index\")\n",
    "        big_data = pd.concat([big_data, result_df])\n",
    "        time.sleep(60.0)\n",
    "\n",
    "    big_data.drop_duplicates(inplace=True)\n",
    "    big_data = big_data.reset_index()\n",
    "    big_data.drop('index', axis=1, inplace = True)\n",
    "    big_data\n",
    "    path = (r\"C:\\Users\\shark\\OneDrive\\Documents\\TwData\\Pickle\" + \"%d\" + \".pkl\")% i\n",
    "    print(path)\n",
    "    big_data.to_pickle(path)\n",
    "# how to use pickle\n",
    "# HW: Try to get more data\n",
    "# Save it in multiple pickle files: make file name pickle + i to get pickle1, pickle2, etc.\n",
    "# Make another loop to do this for each pickle\n",
    "# If I go really high make sure to take off disc storage\n",
    "# Figure out how to deal with disc storage b/c can't add more layers until I do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e432e374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author_id', 'lang', 'text', 'source', 'retweet_count', 'reply_count',\n",
       "       'like_count', 'quote_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
